{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "os.environ['MPLCONFIGDIR'] = '/nfs/nas-6.1/wclu/cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "directory = '/nfs/nas-6.1/wclu/prompt_compression/output'\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    for filename in tqdm(files):\n",
    "        if 'random' in filename:\n",
    "            df = pd.read_json(os.path.join(root, filename))\n",
    "            config = filename.replace('random_','').replace('.json','').split('_')\n",
    "            config.append(df)\n",
    "            result.append(config)\n",
    "df = pd.DataFrame(result, columns=['size','c_rate', 'df'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertscore = load(\"bertscore\")\n",
    "def evaluate_bert(df):\n",
    "    seq_predictions = df['seq_com_output'].tolist()\n",
    "    para_predictions = df['para_com_output'].tolist()\n",
    "    references = df['output'].tolist()\n",
    "    seq_score = bertscore.compute(predictions=seq_predictions, references=references, model_type=\"microsoft/deberta-large-mnli\", batch_size=16)\n",
    "    para_score = bertscore.compute(predictions=para_predictions, references=references, model_type=\"microsoft/deberta-large-mnli\", batch_size=16)\n",
    "    seq_f1 = np.mean(seq_score['f1'])\n",
    "    para_f1 = np.mean(para_score['f1'])\n",
    "    return format(seq_f1,'.4f'), format(para_f1,'.4f')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_bertscore = []\n",
    "para_bertscore = []\n",
    "for i in tqdm(range(len(df))):\n",
    "    bert_score = evaluate_bert(df['df'][i])\n",
    "    seq_bertscore.append(bert_score[0])\n",
    "    para_bertscore.append(bert_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['seq_bertscore'] = seq_bertscore\n",
    "df['para_bertscore'] = para_bertscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = load(\"perplexity\",module_type=\"metric\")\n",
    "def evaluate_perplexity(df, size):\n",
    "    #references = ('User: '+df['prompt']+'\\nAssistant: '+df['output']).tolist()\n",
    "    seq_predictions = (df['prompt']+' '+df['seq_com_output']).tolist()\n",
    "    para_predictions = (df['prompt']+' '+df['para_com_output']).tolist()\n",
    "    #ref_ppl = perplexity.compute(model_id=f'meta-llama/Llama-2-{size}-chat-hf', predictions=references, batch_size=16)\n",
    "    seq_ppl = perplexity.compute(model_id=f'meta-llama/Llama-2-{size}-chat-hf', predictions=seq_predictions, batch_size=32)\n",
    "    para_ppl = perplexity.compute(model_id=f'meta-llama/Llama-2-{size}-chat-hf', predictions=para_predictions, batch_size=32)\n",
    "    #return format(ref_ppl['mean_perplexity'],'.3f'), format(seq_ppl['mean_perplexity'],'.3f'), format(para_ppl['mean_perplexity'],'.3f')\n",
    "    return format(seq_ppl['mean_perplexity'],'.3f'), format(para_ppl['mean_perplexity'],'.3f')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_perplexity = []\n",
    "para_perplexity = []\n",
    "for i in tqdm(range(len(df))):\n",
    "    ppl = evaluate_perplexity(df['df'][i], df['size'][i])\n",
    "    seq_perplexity.append(ppl[0])\n",
    "    para_perplexity.append(ppl[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['seq_ppl'] = seq_perplexity\n",
    "df['para_ppl'] = para_perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['size']=='13b'].sort_values('c_rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['size']=='7b'].sort_values('c_rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = ('A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user\\'s questions.\\nUSER: ' + df['df'][11]['prompt']+'\\nASSISTANT: '+df['df'][11]['seq_com_output']).tolist()\n",
    "#references = ('USER: ' + df['df'][11]['prompt']+'\\nASSISTANT: '+df['df'][11]['seq_com_output']).tolist()\n",
    "print(references[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_ppl = perplexity.compute(model_id=f'lmsys/vicuna-7b-v1.5', predictions=references, batch_size=32)\n",
    "print(format(ref_ppl['mean_perplexity'],'.3f'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(score),5)):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-1106\",\n",
    "        messages=[\n",
    "            {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Imagine that you have a super-intelligent AI assistant, and that you require help with the following question. Which answer best satisfies your needs?\"\n",
    "            },\n",
    "            {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Question: {prompt_list[i]}\\n\\nAnswer G: <{references[i]}>\\n\\nAnswer F: <{seq_predictions[i]}>\\n\\nComparing these two answers, which answer is better?\\n◼ Answer F is significantly better.\\n◼ Answer U is significantly better.\\n◼ Neither is significantly better.\"\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=30,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "    score.append(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
